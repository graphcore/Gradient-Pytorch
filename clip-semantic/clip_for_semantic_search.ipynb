{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "809eb5fd",
   "metadata": {},
   "source": [
    "Copyright (c) 2023 Graphcore Ltd. All rights reserved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c5c6a2",
   "metadata": {},
   "source": [
    "# CLIP for Semantic Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c99fc31-3574-4a88-ad50-50cebaedfa78",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Semantic search is revolutionizing the way we interact with information retrieval systems. It shifts the focus from keyword matching to understanding the intent and contextual meaning behind a search query. This paradigm allows for more intuitive and relevant results, as the system comprehends queries in a human-like manner. The CLIP (Contrastive Language‚ÄìImage Pretraining) model, developed by OpenAI, emerges as a cutting-edge tool in this space. CLIP's unique ability to encode both text and images into a shared representation space enables a deep understanding of content, making it ideal for semantic search tasks.\n",
    "\n",
    "In this tutorial, we will explore how the CLIP model can be utilized for semantic search, beginning with an introduction to the model itself and why it's suited for this task, followed by a deeper dive into the concept of semantics. Later, we will guide you through implementing a fast nearest neighbour search, essential for efficiently retrieving the most semantically relevant results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ad0195-8032-4eeb-b748-9b91cc138559",
   "metadata": {},
   "source": [
    "## CLIP: Connecting text and images\n",
    "\n",
    "The CLIP model is a transformative approach that learns visual concepts from natural language supervision.\n",
    "\n",
    "- Multimodal understanding: CLIP is trained on a diverse range of images and text, enabling it to understand and relate to a wide variety of concepts semantically.\n",
    "\n",
    "- Zero-shot capabilities: It can perform tasks without any task-specific fine-tuning, making it versatile for semantic search across different domains.\n",
    "\n",
    "- Shared embedding space: CLIP maps both text and images into a common embedding space, allowing for direct comparison and relevance matching between text queries and image content.\n",
    "\n",
    "\n",
    "\n",
    "During training, the model uses two encoders: one for images and another for text. The image encoder processes batches of images to produce image embeddings (I‚ÇÅ, I‚ÇÇ, ..., I‚Çô), and the text encoder processes the corresponding captions to produce text embeddings (T‚ÇÅ, T‚ÇÇ, ..., T‚Çô).\n",
    "\n",
    "The goal of contrastive learning here is to align the embeddings from the image encoder with the correct embeddings from the text encoder. For each image embedding, the model calculates its similarity with every text embedding in the batch. The pairs of images and their corresponding captions (I‚ÇÅ-T‚ÇÅ, I‚ÇÇ-T‚ÇÇ, ..., I‚Çô-T‚Çô) should have the highest similarity scores.\n",
    "\n",
    "The similarities between non-matching image-text pairs (for example, I‚ÇÅ-T‚ÇÇ, I‚ÇÇ-T‚ÇÉ) are also calculated, but these should result in lower scores. This contrast between high similarity for matching pairs and low similarity for non-matching pairs is what trains the model to accurately associate images with relevant text.\n",
    "\n",
    "The training process uses these similarities to compute a contrastive loss, such as the triplet loss or noise-contrastive estimation. This loss is minimized when the matching image-text pairs have high similarity scores, and non-matching pairs have low similarity scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to improve usability and support for future users, Graphcore would like to collect information about the\n",
    "applications and code being run in this notebook. The following information will be anonymised before being sent to Graphcore:\n",
    "\n",
    "- User progression through the notebook\n",
    "- Notebook details: number of cells, code being run and the output of the cells\n",
    "- Environment details\n",
    "\n",
    "You can disable logging at any time by running `%unload_ext graphcore_cloud_tools.notebook_logging.gc_logger` from any cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5241c041-0f59-49c3-b495-be4f71698a7d",
   "metadata": {},
   "source": [
    "# How to run CLIP on IPUs\n",
    "\n",
    "Since CLIP checkpoints are available on Hugging Face, you can use the [ü§ó `transformers`](https://huggingface.co/docs/transformers/index) library to load the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e4d8ce-4acf-4698-81da-c7ab9ce5af39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-08T11:20:44.826338Z",
     "iopub.status.busy": "2024-01-08T11:20:44.826131Z",
     "iopub.status.idle": "2024-01-08T11:20:47.086253Z",
     "shell.execute_reply": "2024-01-08T11:20:47.085215Z",
     "shell.execute_reply.started": "2024-01-08T11:20:44.826318Z"
    }
   },
   "outputs": [],
   "source": [
    "%pip install transformers\n",
    "%pip install graphcore-cloud-tools[logger]@git+https://github.com/graphcore/graphcore-cloud-tools@v0.3\n",
    "%load_ext graphcore_cloud_tools.notebook_logging.gc_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f9a471-32cc-4566-9014-a0908eefbb11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-08T11:20:47.088293Z",
     "iopub.status.busy": "2024-01-08T11:20:47.088073Z",
     "iopub.status.idle": "2024-01-08T11:20:49.484533Z",
     "shell.execute_reply": "2024-01-08T11:20:49.483600Z",
     "shell.execute_reply.started": "2024-01-08T11:20:47.088270Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677d9e52-f664-4eb1-8bcf-168dea7238eb",
   "metadata": {},
   "source": [
    "To run this model on IPUs, we first set up the IPU options.\n",
    "\n",
    "The `setAvailableMemoryProportion` option controls how much memory an operation (such as a matmul or convolution) will try to use for temporary values, intermediate sums and so on. It allows us to trade-off between memory and execution time (of operations). The default value is 0.6 i.e. 60% of memory. You can read more about how to set this option in the technical note [Optimising Temporary Memory Usage for Convolutions and Matmuls on the IPU](https://docs.graphcore.ai/projects/available-memory/en/latest/index.html). \n",
    "\n",
    "The `replicationFactor` option controls how many data-parallel models to use. For example, our model uses 1 IPU, so we set the `replicationFactor` to 1. If you are using a POD4, then you can set the `replicationFactor` to 4 to use all 4 available IPUs. \n",
    "\n",
    "We set:\n",
    "- `setAvailableMemoryProportion` to 0.1 to ensure that the model fits in the memory of 1 IPU.\n",
    "- `replicationFactor` to 1 to process a single image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb55825b-705e-4314-9ee7-b2ee17062d7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-08T11:20:49.486346Z",
     "iopub.status.busy": "2024-01-08T11:20:49.485950Z",
     "iopub.status.idle": "2024-01-08T11:20:49.604279Z",
     "shell.execute_reply": "2024-01-08T11:20:49.603582Z",
     "shell.execute_reply.started": "2024-01-08T11:20:49.486327Z"
    }
   },
   "outputs": [],
   "source": [
    "# To run on IPU we import poptorch package\n",
    "import poptorch\n",
    "\n",
    "opts = poptorch.Options()\n",
    "opts.setAvailableMemoryProportion({\"IPU0\": 0.1})  # to fit in IPU memory\n",
    "opts.replicationFactor(1)  # this can be set higher to utilize all available IPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a8c749-60b2-4dd2-b6d3-6542ab2eb8ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T13:12:20.248685Z",
     "iopub.status.busy": "2023-11-22T13:12:20.248508Z",
     "iopub.status.idle": "2023-11-22T13:12:21.794639Z",
     "shell.execute_reply": "2023-11-22T13:12:21.793524Z",
     "shell.execute_reply.started": "2023-11-22T13:12:20.248669Z"
    }
   },
   "source": [
    "The model takes an image and a list of strings as input. \n",
    "\n",
    "We can download a sample image: http://images.cocodataset.org/val2017/000000039769.jpg\n",
    "![an image of a cat](http://images.cocodataset.org/val2017/000000039769.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c860330",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-08T11:20:49.605613Z",
     "iopub.status.busy": "2024-01-08T11:20:49.605232Z",
     "iopub.status.idle": "2024-01-08T11:20:49.806702Z",
     "shell.execute_reply": "2024-01-08T11:20:49.805833Z",
     "shell.execute_reply.started": "2024-01-08T11:20:49.605594Z"
    }
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "# download and open an image\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "texts = [\"a big dog sleeping\", \"two cats sleeping\", \"two cats on a couch\"]\n",
    "\n",
    "# process the input\n",
    "inputs = processor(text=texts, images=image, return_tensors=\"pt\", padding=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7381f91",
   "metadata": {},
   "source": [
    "We wrap the model with `poptorch.inferenceModel` to compile a `torch.nn.Model` model for the IPU.\n",
    "\n",
    "***Warning:*** The first time you run the next cell, the model will be compiled to run on IPUs. This will take approximately 1 minute. Subsequent runs will not trigger a new compilation, and will take less time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5937f551-c9ba-4a35-870a-35464c21a745",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-08T11:20:50.792631Z",
     "iopub.status.busy": "2024-01-08T11:20:50.792427Z",
     "iopub.status.idle": "2024-01-08T11:21:16.891147Z",
     "shell.execute_reply": "2024-01-08T11:21:16.890255Z",
     "shell.execute_reply.started": "2024-01-08T11:20:50.792613Z"
    }
   },
   "outputs": [],
   "source": [
    "# instantiate inferenceModel() by passing model and poptorch options\n",
    "clip_on_ipu = poptorch.inferenceModel(model, options=opts)\n",
    "\n",
    "# run the model\n",
    "outputs = clip_on_ipu(**inputs)  # will compile on first run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ebd880-9453-40e8-90d9-c92c121aa8d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T14:09:26.846241Z",
     "iopub.status.busy": "2023-11-22T14:09:26.845663Z",
     "iopub.status.idle": "2023-11-22T14:09:26.852901Z",
     "shell.execute_reply": "2023-11-22T14:09:26.852271Z",
     "shell.execute_reply.started": "2023-11-22T14:09:26.846216Z"
    }
   },
   "source": [
    "To show the result, we use `matplotlib` to display the image and the similarity score for each sentence with the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcff8f3-cb5b-4d41-b983-e784ef1f2df1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-08T11:23:25.676907Z",
     "iopub.status.busy": "2024-01-08T11:23:25.676396Z",
     "iopub.status.idle": "2024-01-08T11:23:28.045646Z",
     "shell.execute_reply": "2024-01-08T11:23:28.044646Z",
     "shell.execute_reply.started": "2024-01-08T11:23:25.676883Z"
    }
   },
   "outputs": [],
   "source": [
    "%pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e75bf89-b179-4f5d-9a2c-5030b5d6c543",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-08T11:23:28.047419Z",
     "iopub.status.busy": "2024-01-08T11:23:28.047214Z",
     "iopub.status.idle": "2024-01-08T11:23:28.604890Z",
     "shell.execute_reply": "2024-01-08T11:23:28.603565Z",
     "shell.execute_reply.started": "2024-01-08T11:23:28.047399Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "scores = outputs.logits_per_image.softmax(dim=1)[0].numpy()\n",
    "\n",
    "# Create a figure and a set of subplots with adjusted relative heights\n",
    "fig, axs = plt.subplots(\n",
    "    2, 1, figsize=(8, 6), gridspec_kw={\"height_ratios\": [3, 1]}\n",
    ")  # Adjust as needed\n",
    "\n",
    "# Display the image in the first subplot\n",
    "axs[0].imshow(image)\n",
    "axs[0].axis(\"off\")  # Hide the axis for the image\n",
    "\n",
    "# Plot the probabilities as horizontal lines in the second subplot\n",
    "for i, prob in enumerate(scores):\n",
    "    axs[1].hlines(y=i, xmin=0, xmax=prob, color=\"blue\", linewidth=5)\n",
    "    axs[1].set_yticks(range(len(texts)))\n",
    "    axs[1].set_yticklabels(texts)\n",
    "    axs[1].set_ylim(-1, len(texts))  # Adjust for better spacing\n",
    "    axs[1].set_xlim(0, 1)  # Assuming probability values are between 0 and 1\n",
    "    axs[1].set_xlabel(\"Best Match\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a363aa68-e067-4fde-9330-3afb73faefbb",
   "metadata": {},
   "source": [
    "## Semantic search\n",
    "\n",
    "Semantic search goes beyond mere keyword matching; it's about comprehending the searcher's intent and the contextual meaning of their query. This approach allows search engines to deliver results that are more aligned with what the user is actually seeking.\n",
    "\n",
    "At its core, semantic search uses natural language processing (NLP) to understand the intent behind a query. Instead of focusing on the literal words typed into the search box, semantic search considers the query's context and nuances.\n",
    "\n",
    "The CLIP model by OpenAI stands out for its ability to understand both visual and textual content. This makes it especially powerful for semantic search, as it can interpret the meaning behind images and text alike, often capturing subtleties that traditional models might miss.\n",
    "\n",
    "To incorporate semantics into search with CLIP, the process involves:\n",
    "\n",
    "- Processing queries: Transforming user queries into a form that the model can understand, which might include recognizing and disambiguating key terms.\n",
    "\n",
    "- Understanding content: Using the model to generate embeddings that capture the deeper meaning within the database's content.\n",
    "\n",
    "- Matching: Comparing query embeddings to content embeddings to find the best match based on semantic similarity.\n",
    "\n",
    "To do so, we will use the model to extract image and text embeddings and create a FAISS index to do the matching.\n",
    "\n",
    "We first define the image and the text models by isolating the vision part and the text part of the model, with their respective projections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf4ccb9-d03d-4393-8ac5-16aa185e0f98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-08T11:23:28.606869Z",
     "iopub.status.busy": "2024-01-08T11:23:28.606373Z",
     "iopub.status.idle": "2024-01-08T11:23:28.612590Z",
     "shell.execute_reply": "2024-01-08T11:23:28.611945Z",
     "shell.execute_reply.started": "2024-01-08T11:23:28.606847Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class ImageModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Image inference model - use the vision model of CLIP and the visual projection\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        vision_outputs = self.model.vision_model(pixel_values)\n",
    "        return self.model.visual_projection(vision_outputs[1])\n",
    "\n",
    "\n",
    "class TextModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Text inference model - use the text model and the text projection\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "    def forward(self, **kargs):\n",
    "        text_outputs = self.model.text_model(**kargs)\n",
    "        return self.model.text_projection(text_outputs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c02babb-8a07-4a28-95e6-4373205f5a67",
   "metadata": {},
   "source": [
    "We first need to install [faiss](https://github.com/facebookresearch/faiss) (the CPU version) and the [ü§ó `datasets`](https://huggingface.co/docs/datasets/index) library to download some images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ef4f53-69fa-4c03-bdbb-637cc88ba562",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-08T11:23:31.011086Z",
     "iopub.status.busy": "2024-01-08T11:23:31.010441Z",
     "iopub.status.idle": "2024-01-08T11:23:33.415792Z",
     "shell.execute_reply": "2024-01-08T11:23:33.414798Z",
     "shell.execute_reply.started": "2024-01-08T11:23:31.011064Z"
    }
   },
   "outputs": [],
   "source": [
    "%pip install faiss-cpu datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579dcfa1-56af-4fbd-979d-eda125befd45",
   "metadata": {},
   "source": [
    "In this example, we will use the [Human Action Recognition (HAR)](https://huggingface.co/datasets/Bingsu/Human_Action_Recognition) image dataset.\n",
    "\n",
    "Here, you can use your own dataset or any other image dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba35ae5d-11be-480a-9e5b-725f5e1a0619",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-08T11:23:33.417852Z",
     "iopub.status.busy": "2024-01-08T11:23:33.417381Z",
     "iopub.status.idle": "2024-01-08T11:23:34.924866Z",
     "shell.execute_reply": "2024-01-08T11:23:34.923906Z",
     "shell.execute_reply.started": "2024-01-08T11:23:33.417830Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"Bingsu/Human_Action_Recognition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9c5d1b-80cc-49d1-a611-081bd5e9d251",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-08T11:23:35.815035Z",
     "iopub.status.busy": "2024-01-08T11:23:35.814484Z",
     "iopub.status.idle": "2024-01-08T11:23:36.008689Z",
     "shell.execute_reply": "2024-01-08T11:23:36.007489Z",
     "shell.execute_reply.started": "2024-01-08T11:23:35.815014Z"
    }
   },
   "outputs": [],
   "source": [
    "clip_on_ipu.detachFromDevice()  # detach the previous model to free up IPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab19d08-b8d4-4e0d-a881-aa8adc991cb6",
   "metadata": {},
   "source": [
    "For each image in the dataset, we extract its embedding representation. \n",
    "We can now set the `replicationFactor` to a higher number, to speed up the inference time over the dataset.\n",
    "\n",
    "You can use : `os.getenv(\"NUM_AVAILABLE_IPU\")` to get the number of available IPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f1ed4b-a0bb-428a-b29b-d5987790c10c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-08T11:42:32.924371Z",
     "iopub.status.busy": "2024-01-08T11:42:32.923821Z",
     "iopub.status.idle": "2024-01-08T11:43:54.077362Z",
     "shell.execute_reply": "2024-01-08T11:43:54.075282Z",
     "shell.execute_reply.started": "2024-01-08T11:42:32.924346Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "num_available_ipus = int(os.getenv(\"NUM_AVAILABLE_IPU\", 4))\n",
    "\n",
    "opts = poptorch.Options()  # we instantiate new poptorch options\n",
    "opts.setAvailableMemoryProportion({\"IPU0\": 0.1})  # to fit in IPU memory\n",
    "opts.replicationFactor(\n",
    "    num_available_ipus\n",
    ")  # this can be set to 4 to utilize all available IPUs\n",
    "\n",
    "im = poptorch.inferenceModel(\n",
    "    ImageModel(), options=opts\n",
    ")  # Wrap image model to compile for IPU\n",
    "\n",
    "\n",
    "def process_batch(batch):\n",
    "    \"\"\"\n",
    "    For each batch, we need to process it and store each output in the new column embeddings\n",
    "    \"\"\"\n",
    "    # Process the images in the batch\n",
    "    inputs = processor(images=batch[\"image\"], return_tensors=\"pt\")\n",
    "\n",
    "    # Check if padding is necessary\n",
    "    nb_padding = num_available_ipus - inputs[\"pixel_values\"].shape[0]\n",
    "    if nb_padding > 0:\n",
    "        inputs[\"pixel_values\"] = nn.functional.pad(\n",
    "            inputs[\"pixel_values\"], (0, 0, 0, 0, 0, 0, 0, nb_padding), \"constant\", 0\n",
    "        )\n",
    "\n",
    "    # Apply the model and convert to numpy\n",
    "    embeddings = [\n",
    "        output.numpy() for output in im(**inputs)[: num_available_ipus - nb_padding]\n",
    "    ]\n",
    "    return {\"embeddings\": embeddings}\n",
    "\n",
    "\n",
    "# Apply the function to the dataset in a batched manner\n",
    "ds_with_embeddings = dataset[\"test\"].map(\n",
    "    process_batch, batched=True, batch_size=num_available_ipus\n",
    ")  # should take around one minute and a half with 4 IPUs\n",
    "\n",
    "im.detachFromDevice()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8114ed-5716-4287-851d-768572b52ec1",
   "metadata": {},
   "source": [
    "We use the built-in function `add_faiss_index()` to create the index with the newly created column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707cabd1-b477-4397-9608-a040849fec46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-08T11:45:31.852475Z",
     "iopub.status.busy": "2024-01-08T11:45:31.851656Z",
     "iopub.status.idle": "2024-01-08T11:45:31.919176Z",
     "shell.execute_reply": "2024-01-08T11:45:31.918357Z",
     "shell.execute_reply.started": "2024-01-08T11:45:31.852448Z"
    }
   },
   "outputs": [],
   "source": [
    "ds_with_embeddings.add_faiss_index(column=\"embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d24519-37ea-46e5-ad7e-ca32589fba1c",
   "metadata": {},
   "source": [
    "Let's test our approach by choosing a random image from the test dataset and retrieving its closest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02c14b5-2c51-4df8-8acd-92073a2657c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-08T11:45:33.291743Z",
     "iopub.status.busy": "2024-01-08T11:45:33.291148Z",
     "iopub.status.idle": "2024-01-08T11:45:33.304799Z",
     "shell.execute_reply": "2024-01-08T11:45:33.304230Z",
     "shell.execute_reply.started": "2024-01-08T11:45:33.291717Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "image_test = dataset[\"test\"][random.randint(0, len(dataset[\"test\"]))][\"image\"]\n",
    "image_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bacc2a9-1ce9-433e-b92d-7e26a246d022",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-08T11:45:35.327025Z",
     "iopub.status.busy": "2024-01-08T11:45:35.326461Z",
     "iopub.status.idle": "2024-01-08T11:46:28.625552Z",
     "shell.execute_reply": "2024-01-08T11:46:28.624547Z",
     "shell.execute_reply.started": "2024-01-08T11:45:35.327002Z"
    }
   },
   "outputs": [],
   "source": [
    "opts = poptorch.Options()\n",
    "opts.setAvailableMemoryProportion({\"IPU0\": 0.1})\n",
    "opts.replicationFactor(1)\n",
    "\n",
    "im = poptorch.inferenceModel(ImageModel(), options=opts)\n",
    "\n",
    "inputs = processor(images=image_test, return_tensors=\"pt\")  # process the input\n",
    "outputs = im(**inputs)  # compute input image embedding\n",
    "scores, retrieved_examples = ds_with_embeddings.get_nearest_examples(\n",
    "    \"embeddings\", outputs[0].numpy(), k=10\n",
    ")  # find 10 nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8a619f-a7f5-4d68-8074-2e4988063471",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-08T11:46:28.627454Z",
     "iopub.status.busy": "2024-01-08T11:46:28.627155Z",
     "iopub.status.idle": "2024-01-08T11:46:29.152233Z",
     "shell.execute_reply": "2024-01-08T11:46:29.151225Z",
     "shell.execute_reply.started": "2024-01-08T11:46:28.627435Z"
    }
   },
   "outputs": [],
   "source": [
    "images = retrieved_examples[\"image\"]\n",
    "\n",
    "# Create a figure and subplots\n",
    "fig, axs = plt.subplots(2, 5, figsize=(15, 10))\n",
    "\n",
    "# Flatten the array of axes, for easy iterating\n",
    "axs = axs.flatten()\n",
    "\n",
    "for i, img in enumerate(images):\n",
    "    axs[i].imshow(img)\n",
    "    axs[i].axis(\"off\")  # Hide axes\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb80c0b2-27af-4cc4-a5a6-36441c839990",
   "metadata": {},
   "source": [
    "We can try the same with some text input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c92200-6583-4901-91ce-3af6b1660cac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-06T10:57:01.282303Z",
     "iopub.status.busy": "2023-12-06T10:57:01.281761Z",
     "iopub.status.idle": "2023-12-06T10:57:37.983879Z",
     "shell.execute_reply": "2023-12-06T10:57:37.982938Z",
     "shell.execute_reply.started": "2023-12-06T10:57:01.282280Z"
    }
   },
   "outputs": [],
   "source": [
    "tm = poptorch.inferenceModel(TextModel(), options=opts)\n",
    "inputs = processor(text=[\"a person looking at their phone\"], return_tensors=\"pt\")\n",
    "outputs = tm(\n",
    "    **inputs\n",
    ")  # this will compile the TextModel the first time it is called, this takes ~30s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a397a68-da2f-486d-b8c3-7a1b1b1ed0cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-06T10:57:37.985942Z",
     "iopub.status.busy": "2023-12-06T10:57:37.985346Z",
     "iopub.status.idle": "2023-12-06T10:57:37.999361Z",
     "shell.execute_reply": "2023-12-06T10:57:37.997787Z",
     "shell.execute_reply.started": "2023-12-06T10:57:37.985923Z"
    }
   },
   "outputs": [],
   "source": [
    "scores, retrieved_examples = ds_with_embeddings.get_nearest_examples(\n",
    "    \"embeddings\", outputs[0].numpy(), k=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dec26b7-168a-45b5-a9c2-a795c876bc4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-06T10:57:38.000877Z",
     "iopub.status.busy": "2023-12-06T10:57:38.000424Z",
     "iopub.status.idle": "2023-12-06T10:57:38.574321Z",
     "shell.execute_reply": "2023-12-06T10:57:38.573583Z",
     "shell.execute_reply.started": "2023-12-06T10:57:38.000852Z"
    }
   },
   "outputs": [],
   "source": [
    "images = retrieved_examples[\"image\"]\n",
    "\n",
    "# Create a figure and subplots\n",
    "fig, axs = plt.subplots(2, 5, figsize=(15, 10))  # Adjust the figsize as needed\n",
    "\n",
    "# Flatten the array of axes, for easy iterating\n",
    "axs = axs.flatten()\n",
    "\n",
    "for i, img in enumerate(images):\n",
    "    axs[i].imshow(img)\n",
    "    axs[i].axis(\"off\")  # Hide axes\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997c7588-c8da-471e-9129-45e0e39a0fb0",
   "metadata": {},
   "source": [
    "## Going further: Fine-tuning the model\n",
    "\n",
    "\n",
    "\n",
    "A great way to improve results on a specific task is to fine-tune the model. Fine-tuning a model like CLIP with user feedback, such as which images users clicked on, involves a process of retraining the model (or parts of it) using a dataset enriched with this feedback. The goal is to adjust the model's parameters so that it better aligns with users' implicit indications of relevance.\n",
    "\n",
    "Here's a high-level approach to fine-tuning the CLIP model with user feedback:\n",
    "\n",
    "- Gather data on user interactions. For instance, when a user searches using a text query and clicks on certain images, log the query, the images displayed, and which images were clicked. This data implies that the clicked images were more relevant to the user's query than those that were not clicked.\n",
    "\n",
    "- Construct a new dataset where each instance consists of the user's query, the selected (clicked) image, and potentially the images that were not selected. The selected images can be treated as positive examples, while the non-selected images, shown in the same search result, can be used as negative examples.\n",
    "\n",
    "- Decide on a fine-tuning strategy. You might choose to fine-tune the entire model or only specific layers. For instance, you may only fine-tune the final layers responsible for matching text and image embeddings.\n",
    "\n",
    "- Modify the training objective to incorporate the feedback. One common approach is to use a ranking loss function that pushes the model to rank clicked (relevant) images higher than non-clicked (irrelevant) images for a given query.\n",
    "\n",
    "\n",
    "\n",
    "An example of training CLIP on IPU is available on [GitHub](https://github.com/graphcore/examples/blob/master/multimodal/CLIP/pytorch/train.py). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
