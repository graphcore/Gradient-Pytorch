{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2b361bf",
   "metadata": {},
   "source": [
    "Copyright (c) 2023 Graphcore Ltd. All rights reserved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdeae4e8",
   "metadata": {},
   "source": [
    "# Object Detection with YOLO v4 model executed on IPU\n",
    "This notebook demonstrates the object detection task realized using YOLOv4 model inference pipeline run on Graphcore IPUs. Originally, the code of [YOLOv4 model adapted to IPU was published in examples github repository](https://github.com/graphcore/examples/tree/master/vision/yolo_v4/pytorch).\n",
    "\n",
    "### Summary table\n",
    "\n",
    "|  Domain | Tasks | Model | Datasets | Workflow |   Number of IPUs   | Execution time |\n",
    "|---------|-------|-------|----------|----------|--------------|--------------|\n",
    "| vision  | object detection | YOLO v4 | COCO | inference | recommended: 1 (min: 4) | 2mn    |\n",
    "\n",
    "\n",
    "![object detection on IPU](notebook/first_example.png \"Last supper object detection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db890f52",
   "metadata": {},
   "source": [
    "\n",
    "## Environment setup\n",
    "\n",
    "The best way to run this demo is on Paperspace Gradient's cloud IPUs because everything is already set up for you. To run the demo using other IPU hardware, you need to have the Poplar SDK enabled and the relevant PopTorch wheels installed. Refer to the [getting started guide](https://docs.graphcore.ai/en/latest/getting-started.html#getting-started) for your system for details on how to enable the Poplar SDK and install the PopTorch wheels.\n",
    "\n",
    "\n",
    "## Requirements\n",
    "Before using the model on IPU you have to build the custom operations for IPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c567f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!make"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b68706",
   "metadata": {},
   "source": [
    "and install the Python dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c72d24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0737bab",
   "metadata": {},
   "source": [
    "## COCO dataset\n",
    "\n",
    "This demonstration example of inference with YOLO v4 model is using the checkpoint of model trained with [COCO dataset](https://cocodataset.org/). Therefore, we can demonstrate detection of 80 different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fd2135",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ruamel import yaml\n",
    "\n",
    "class_names = yaml.safe_load(open(\"configs/class_name.yaml\"))[\"class_names\"]\n",
    "class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33a842f",
   "metadata": {},
   "source": [
    "## Model preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5e71f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from models.yolov4_p5 import Yolov4P5\n",
    "from notebook.yolo_ipu_pipeline import YOLOv4InferencePipeline\n",
    "\n",
    "import poptorch\n",
    "\n",
    "poptorch.setLogLevel(4)  # ERR log level\n",
    "\n",
    "path_to_detection = Path().parent.resolve()\n",
    "os.environ[\"PYTORCH_APPS_DETECTION_PATH\"] = str(path_to_detection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e23e74",
   "metadata": {},
   "source": [
    "We use the original YOLOv4 model with checkpoint trained with COCO dataset, which we pass as `checkpoint`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aaded57",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Yolov4P5\n",
    "checkpoint = \"checkpoint/yolov4_p5_reference_weights/yolov4-p5-sd.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63d8db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "FILE=./checkpoint/yolov4_p5_reference_weights/yolov4-p5-sd.pt; \\\n",
    "if [ -f \"$FILE\" ]; then \\\n",
    "    echo \"$FILE exists, no need to download.\"; \\\n",
    "else \\\n",
    "    mkdir checkpoint; \\\n",
    "    cd checkpoint; \\\n",
    "    curl https://gc-demo-resources.s3.us-west-1.amazonaws.com/yolov4_p5_reference_weights.tar.gz -o yolov4_p5_reference_weights.tar.gz && tar -zxvf yolov4_p5_reference_weights.tar.gz && rm yolov4_p5_reference_weights.tar.gz; \\\n",
    "    cd ..; \\\n",
    "fi "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0d4c56",
   "metadata": {},
   "source": [
    "We use a `YOLOv4InferencePipeline` class to set all IPU specific options and wrap the PyTorch model into Graphcore PopTorch inference model. The pipeline reads the configuration parameters from the [config file](configs/override-inference-yolov4p5.yaml)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9261c334",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = YOLOv4InferencePipeline(\n",
    "    model=Yolov4P5,\n",
    "    checkpoint_path=\"checkpoint/yolov4_p5_reference_weights/yolov4-p5-sd.pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baac6d44",
   "metadata": {},
   "source": [
    "We demonstrate the inference on an exemplary image of famous painting, which is stored in the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ab03a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(\"notebook/last_supper_restored.jpeg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7ca25a",
   "metadata": {},
   "source": [
    "The pipeline call consists of preprocessing, forward pass of the preprocessed image through the model and model output postprocessing.\n",
    "When used for the first time, a one-time compilation of the model is triggered.\n",
    "For a single IPU it should take about 3 min for the first time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abf9a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "processed_batch = pipeline(image)\n",
    "\n",
    "inference_time = time.time() - start_time\n",
    "print(\"pipeline inference time: \", inference_time * 10e3, \"msec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712092f6",
   "metadata": {},
   "source": [
    "You can now check what exactly has been detected on the input image.\n",
    "The output contains a list of detected objects for each image in batch.\n",
    "The result contain five numbers: first four are the coordinates in XYWH format (x, y, width, height of bounding box), and the fifth number represent the predicted class of detected object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f63953",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = [\"x\", \"y\", \"width\", \"height\", \"class\"]\n",
    "objects_detected = []\n",
    "for obj in processed_batch[0][0].tolist():\n",
    "    objects_detected.append({key: value for key, value in zip(keys, obj)})\n",
    "    objects_detected[-1][\"class\"] = class_names[int(objects_detected[-1][\"class\"])]\n",
    "\n",
    "objects_detected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a0858c",
   "metadata": {},
   "source": [
    "Also you can use the coordinates for the detected objects to plot the bounding boxes on the original image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e314726b",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_paths = pipeline.plotImg(processed_batch, image)\n",
    "img_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3de8ba",
   "metadata": {},
   "source": [
    "## Try it on your own\n",
    "\n",
    "Let's check the YOLOv4 model on IPU with some image from the internet.\n",
    "\n",
    "We may use wget command to store the image as `image.png`. The syntax is `!wget -O image.png [YOUR URL]`, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b0739a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPLOAD YOUR OWN IMAGE HERE BY REPLACING THE URL\n",
    "\n",
    "!wget -O image.png https://www.graphcore.ai/hubfs/assets/images/content/new-team.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318ddd7b",
   "metadata": {},
   "source": [
    "Let's now use this image for inference with the same model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b60e451",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "image = Image.open(\"image.png\")\n",
    "\n",
    "processed_batch = pipeline(image)\n",
    "img_paths = pipeline.plotImg(processed_batch, image)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(\"Total time until plot: \", total_time, \"sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2bae0a",
   "metadata": {},
   "source": [
    "### Optional - Release IPUs in use\n",
    "\n",
    "The IPython kernel has a lock on the IPUs used in running the model, preventing other users from using them. For example, if you wish to use other notebooks after working your way through this one, it may be necessary to manually run the below cell to release IPUs from use. This will happen by default if using the Run All option. More information on the topic can be found at [Managing IPU Resources](https://github.com/gradient-ai/Graphcore-HuggingFace/blob/main/useful-tips/managing_ipu_resources.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4f685f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e7a371",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
