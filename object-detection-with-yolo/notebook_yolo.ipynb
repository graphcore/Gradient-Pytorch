{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Copyright (c) 2023 Graphcore Ltd. All rights reserved."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Object detection on IPU using YOLO v4 - inference\n",
    "This notebook demonstrates an object detection task with a YOLO v4 model using an inference pipeline run on Graphcore IPUs. Originally the code of the YOLO v4 model adapted for IPU was published in the [examples GitHub repository](https://github.com/graphcore/examples/tree/master/vision/yolo_v4/pytorch).\n",
    "\n",
    "### Summary table\n",
    "\n",
    "|  Domain | Tasks | Model | Datasets | Workflow |   Number of IPUs   | Execution time |\n",
    "|---------|-------|-------|----------|----------|--------------|--------------|\n",
    "| Vision  | Object detection | YOLO v4 | COCO | Inference | Recommended: POD4 | 2 minutes    |\n",
    "\n",
    "\n",
    "![object detection on IPU](notebook/first_example.png \"Last supper object detection\")"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Environment setup\n",
    "\n",
    "The best way to run this demo is on Paperspace Gradient's cloud IPUs because everything is already set up for you. \n",
    "[![Run on Gradient](https://assets.paperspace.io/img/gradient-badge.svg)](https://ipu.dev/Y8yaSU)\n",
    "To run the demo using other IPU hardware, you need to have the Poplar SDK enabled and the relevant PopTorch wheels installed. Refer to the [getting started guide](https://docs.graphcore.ai/en/latest/getting-started.html#getting-started) for your system for details on how to enable the Poplar SDK and install the PopTorch wheels.\n",
    "\n",
    "\n",
    "## Requirements\n",
    "Before using the model on IPU you have to build the custom operations for IPU:"
   ],
   "metadata": {},
   "attachments": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!make"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "and install the Python dependencies:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%pip install -r requirements.txt\n",
    "from examples_utils import notebook_logging\n",
    "\n",
    "%load_ext gc_logger"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In order to improve usability and support for future users, Graphcore would like to collect information about the applications and code being run in this notebook. The following information will be anonymised before being sent to Graphcore:\n",
    "\n",
    "- User progression through the notebook\n",
    "- Notebook details: number of cells, code being run and the output of the cells\n",
    "- Environment details\n",
    "\n",
    "You can disable logging at any time by running `%unload_ext gc_logger` from any cell."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## COCO dataset\n",
    "\n",
    "This demo of inference with YOLO v4 on IPU uses the checkpoint from a model trained with the [COCO dataset](https://cocodataset.org/). With this we can demonstrate the detection of 80 different classes."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from ruamel import yaml\n",
    "\n",
    "class_names = yaml.safe_load(open(\"configs/class_name.yaml\"))[\"class_names\"]\n",
    "class_names"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model preparation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "\n",
    "import poptorch\n",
    "\n",
    "path_to_detection = Path().parent.resolve()\n",
    "os.environ[\"PYTORCH_APPS_DETECTION_PATH\"] = str(path_to_detection)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We are using the original YOLO v4 model with a checkpoint trained with the COCO dataset, which we pass as `checkpoint`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "checkpoint = \"checkpoint/yolov4_p5_reference_weights/yolov4-p5-sd.pt\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next we download the checkpoint for the model:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%bash\n",
    "\n",
    "FILE=./checkpoint/yolov4_p5_reference_weights/yolov4-p5-sd.pt; \\\n",
    "if [ -f \"$FILE\" ]; then \\\n",
    "    echo \"$FILE exists, no need to download.\"; \\\n",
    "else \\\n",
    "    mkdir checkpoint; \\\n",
    "    cd checkpoint; \\\n",
    "    curl https://gc-demo-resources.s3.us-west-1.amazonaws.com/yolov4_p5_reference_weights.tar.gz -o yolov4_p5_reference_weights.tar.gz && tar -zxvf yolov4_p5_reference_weights.tar.gz && rm yolov4_p5_reference_weights.tar.gz; \\\n",
    "    cd ..; \\\n",
    "fi "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We are using a `YOLOv4InferencePipeline` class to set all the IPU specific options and wrap the PyTorch model in a PopTorch inference model. The pipeline reads the configuration parameters from the [config file](configs/inference-yolov4p5.yaml)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from api import YOLOv4InferencePipeline\n",
    "\n",
    "pipeline = YOLOv4InferencePipeline(\n",
    "    checkpoint_path=\"checkpoint/yolov4_p5_reference_weights/yolov4-p5-sd.pt\",\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We are using the image of a famous painting (The Last Supper by Leonardo da Vinci) to demonstrate how to use the inference model. This image is stored in the repo:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "image = Image.open(\"notebook/last_supper_restored.jpeg\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The pipeline call carries out the following steps: image preprocessing; a forward pass of the preprocessed image through the model; model output postprocessing.\n",
    "When used for the first time, a one-time compilation of the model is triggered.\n",
    "For a single IPU it should take about 3 minutes to compile the model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\n",
    "processed_batch = pipeline(image)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Subsequent calls are much faster:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\n",
    "processed_batch = pipeline(image)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can now check what has been detected on the input image.\n",
    "The output contains a list of detected objects for each image in the batch.\n",
    "Each result contains five numbers: the first four are the coordinates in XYWH format (x, y, width, and height of bounding box), the fifth number represents the confidence of the prediction, and the sixth number represents the predicted class of the detected object."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "processed_batch"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can use the coordinates from the of the detected objects to plot bounding boxes on the original image."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "img_paths = pipeline.plot_img(processed_batch, image)\n",
    "img_paths"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Try it on your own\n",
    "\n",
    "Let's check the YOLOv4 model on IPU with some images from the Internet.\n",
    "\n",
    "We can use the `wget` command to store the image as `image.png`. The syntax is `!wget -O image.png [YOUR URL]`, for example:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# UPLOAD YOUR OWN IMAGE HERE BY REPLACING THE URL\n",
    "\n",
    "!wget -O image.png https://www.graphcore.ai/hubfs/assets/images/content/new-team.jpg\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's now use this image for inference with the same model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\n",
    "image = Image.open(\"image.png\")\n",
    "\n",
    "processed_batch = pipeline(image)\n",
    "img_paths = pipeline.plot_img(processed_batch, image)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Optional - Release IPUs in use\n",
    "\n",
    "The IPython kernel has a lock on the IPUs used in running the model, preventing other users from using them. For example, if you wish to use other notebooks after working your way through this one, it may be necessary to manually run the below cell to release IPUs from use. This will happen by default if using the Run All option. More information on the topic can be found at [Managing IPU Resources](https://github.com/gradient-ai/Graphcore-HuggingFace/blob/main/useful-tips/managing_ipu_resources.ipynb)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pipeline.detach()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}