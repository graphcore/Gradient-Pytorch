{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6b596ece",
   "metadata": {},
   "source": [
    "# BERT-Large Fine Tuning on the IPU\n",
    "\n",
    "This notebook will demonstrate how to fine-tune a pre-trained BERT model with PyTorch on the Graphcore IPU-POD16 and IPU-POD4 system. We will use a BERT-Large model and fine-tune on the SQuADv1 Question/Answering task.\n",
    "\n",
    "We will show how to take a BERT model written in PyTorch from the ðŸ¤—`transformers` library from HuggingFace and parallelize and run it on IPU using PopTorch.\n",
    "\n",
    "This is an advanced tutorial: if you are new to PopTorch, we have tutorials available in the `../learning-PyTorch-on-IPU` folder.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c384dcc7",
   "metadata": {},
   "source": [
    "### Running on Paperspace\n",
    "\n",
    "The Paperspace environment lets you run this notebook with no set up. To improve your experience we preload datasets and pre-install packages, this can take a few minutes, if you experience errors immediately after starting a session please try restarting the kernel before contacting support. If a problem persists or you want to give us feedback on the content of this notebook, please reach out to through our community of developers using our [slack channel](https://www.graphcore.ai/join-community) or raise a [GitHub issue](https://github.com/gradient-ai/Graphcore-Pytorch/issues)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "247bb77a",
   "metadata": {},
   "source": [
    "Requirements:\n",
    "- Python packages installed with `python -m pip install -r requirements.txt`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a4121f58",
   "metadata": {},
   "source": [
    "\n",
    "In order to improve usability and support for future users, Graphcore would like to collect information about the\n",
    "applications and code being run in this notebook. The following information will be anonymised before being sent to Graphcore:\n",
    "\n",
    "- User progression through the notebook\n",
    "- Notebook details: number of cells, code being run and the output of the cells\n",
    "- Environment details\n",
    "\n",
    "You can disable logging at any time by running `%unload_ext graphcore_cloud_tools.notebook_logging.gc_logger` from any cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129e0332",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q -r requirements.txt\n",
    "%load_ext graphcore_cloud_tools.notebook_logging.gc_logger"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "14bcb780",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "BERT fine-tuning is when you train a BERT model on a supervised learning task on a relatively small amount of data, by using starting weights obtained from pre-training on a large, generic text corpus. Pre-training of BERT requires a lot of unlabelled data (for instance all of Wikipedia + thousands of books) and a lot of compute. It is expensive and time-consuming, but after pre-training BERT will have learned an extremely good language model that can be fine-tuned on downstream tasks with small amount of labelled data, achieving great results.\n",
    "\n",
    "![bert.png](static/bert.png)\n",
    "\n",
    "In this notebook we will fine-tune BERT (pre-trained on IPU with the Wikipedia dataset) on a question answering task called SQuAD. Then we will perform inference on the accompanying validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22b1730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard packages\n",
    "import transformers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from datasets import load_dataset, load_metric\n",
    "import time\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# To run on IPU we import popart and poptorch packages\n",
    "import popart\n",
    "import poptorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d6a825",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e500ebb3",
   "metadata": {},
   "source": [
    "## 1. Get the data\n",
    "\n",
    "\n",
    "**What is SQuAD?**\n",
    "\n",
    "> Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.\n",
    "\n",
    "From <https://rajpurkar.github.io/SQuAD-explorer/>\n",
    "\n",
    "Basically you train a model to take a question and read a passage of text and predict the start and end positions of where that answer lies in the passage. The image below shows an example from the dataset:\n",
    "\n",
    "![squad.png](static/squad.png)\n",
    "\n",
    "(Source: [Rajpurkar GitHub](https://rajpurkar.github.io/SQuAD-explorer/explore/1.1/dev/Normans.html))\n",
    "\n",
    "For the case of SQuADv1 there are no unanswerable questions in the dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e4eb4f06",
   "metadata": {},
   "source": [
    "We use the ðŸ¤— `datasets` package to automatically download the SQuAD dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbb77b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = load_dataset(\"squad\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f67a0928",
   "metadata": {},
   "source": [
    "The SQuAD dataset consists of pre-defined training and validation splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d569eb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "004d8c4c",
   "metadata": {},
   "source": [
    "Each row in the data consists of a passage of text - `context` - a question about the passage - `question` - and the answer(s) to the question - `answers`. The latter consists of the text in the passage and the start position in the text.\n",
    "\n",
    "Here is an example row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd80bea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets[\"train\"][10016]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "98f0397a",
   "metadata": {},
   "source": [
    "**How do we preprocess this data to train it with a deep learning model?**\n",
    "\n",
    "We need to `tokenize` the text to turn it from words into numbers. This is done using `transformers.BertTokenizer`. Let's use this to tokenize a shortened version of the example above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec520443",
   "metadata": {},
   "outputs": [],
   "source": [
    "from squad_preprocessing import tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03025be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = {\n",
    "    \"context\": \"Institutes of technology in Venezuela were developed in the 1950s\",\n",
    "    \"question\": \"When were Institutes of technology developed?\",\n",
    "}\n",
    "tokenized_example = tokenizer(\n",
    "    example[\"question\"],\n",
    "    example[\"context\"],\n",
    "    truncation=\"only_second\",\n",
    "    max_length=32,\n",
    "    stride=16,\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True,\n",
    "    padding=\"max_length\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44f57a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_example.keys()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "865b4770",
   "metadata": {},
   "source": [
    "Let's look at the `input_ids`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b692d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_example.input_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee31492",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(tokenized_example.input_ids[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "352125f9",
   "metadata": {},
   "source": [
    "As you can see in the decoded version, the question is placed at the start followed by a `[SEP]` token, then the context, followed by padding if required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72fe3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from squad_preprocessing import (\n",
    "    prepare_train_features,\n",
    "    prepare_validation_features,\n",
    "    tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede36c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets[\"train\"].map(\n",
    "    prepare_train_features,\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    "    remove_columns=datasets[\"train\"].column_names,\n",
    "    load_from_cache_file=True,\n",
    ")\n",
    "\n",
    "# Create validation features from dataset\n",
    "validation_features = datasets[\"validation\"].map(\n",
    "    prepare_validation_features,\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    "    remove_columns=datasets[\"validation\"].column_names,\n",
    "    load_from_cache_file=True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "28672d76",
   "metadata": {},
   "source": [
    "## 2. Get the Model and Parallelize on IPU-POD4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "645959e3",
   "metadata": {},
   "source": [
    "We run the model on IPU using **pipelining** and learn about **data parallelism** in order to maximise hardware use.\n",
    "\n",
    "\n",
    "### Parallelism through pipelining\n",
    "\n",
    "The model layers are split over 4 IPUs. We then use [*pipeline parallelism*](https://docs.graphcore.ai/projects/tf-model-parallelism/page/pipelining.html) over the IPUs with gradient accumulation. We subdivide the compute batch into micro-batches that pass through the pipeline in the forward pass and then come back again in the backwards pass, accumulating gradients for the parameters as they go.\n",
    "\n",
    "A complete pipeline step has a ramp-up phase the start and a ramp-down phase at the end. Increasing the gradient accumulation factor, increases the total batch size and also increases the pipeline efficiency, and therefore throughput, because the proportion of time in ramp-up/down phases will be reduced. \n",
    "\n",
    "![pipelining.png](static/pipelining.png)\n",
    "\n",
    "### Partitioning the Model\n",
    "\n",
    "BERT Large has 24 transformer layers, which we will split over our 4 IPUs. The position and word embeddings, and the first 3 encoder layers will sit on IPU0, the following 3 IPUs have 7 transformer layers each.\n",
    "\n",
    "![bert-pipelining-pod4.png](static/bert-pipelining-pod4.png)\n",
    "\n",
    "### Data Parallelism\n",
    "\n",
    "If running on an IPU-POD4 we cannot use replication to do data parallelism because this system contains 4 IPUs and our pipeline is 4 IPUs long. Therefore, we will have a \"1x4 pipeline\" configuration. For replication of this model, we must acquire a larger IPU-POD system. Using an IPU-POD16 we can move the optimiser on-chip to speed up the model (see below) and rearrange our model to fit across 4 IPU's. This will allow us to create 2 replicas, and feed 2 different micro batches in parallel to create a \"2x8 pipeline\" configuration.\n",
    "\n",
    "### Recomputation Checkpoints\n",
    "\n",
    "We can make more efficient use of the valuable In-Processor-Memory by saving only selected activation inputs and recomputing the rest. This lets us optimise on memory savings (by not storing all activations) vs FLOP expenditure (by not having to recompute all activations). \n",
    "![recomputation.png](static/recomputation.png)\n",
    "Source: [TensorFlow Model Parallelism: Recomputation](https://docs.graphcore.ai/projects/tf-model-parallelism/page/pipelining.html#recomputation)\n",
    "\n",
    "Checkpoints are automatically placed between each pipeline stage. In addition to these automatic checkpoints, we are adding one at the end of every transformer layer, which leads to better performance.\n",
    "\n",
    "### Replicated Tensor Sharding of Optimizer State\n",
    "\n",
    "If we are using multiple replicas, we can also distribute our optimizer state to reduce local memory usage, a method called [On-chip Replicated Tensor Sharding (RTS)](https://docs.graphcore.ai/projects/graphcore-glossary/en/latest/index.html#term-Replicated-tensor-sharding).\n",
    "\n",
    "> To further improve memory availability we also have the option to store tensors in the POD-IPU16 Streaming Memory at the cost of increased communications.\n",
    "\n",
    "![rts.png](static/rts.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0d10e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function marks a PyTorch layer as a recomputation checkpoint\n",
    "def checkpoint_outputs(module: nn.Module):\n",
    "    \"\"\"Annotates the output of a module to be checkpointed instead of\n",
    "    recomputed\"\"\"\n",
    "\n",
    "    def recompute_outputs(module, inputs, outputs):\n",
    "        return tuple(poptorch.recomputationCheckpoint(y) for y in outputs)\n",
    "\n",
    "    module.register_forward_hook(recompute_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf15fe19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer_ipu(layers_per_ipu):\n",
    "    # List of the IPU Id for each encoder layer\n",
    "    layer_ipu = []\n",
    "    for ipu, n_layers in enumerate(layers_per_ipu):\n",
    "        layer_ipu += [ipu] * n_layers\n",
    "    return layer_ipu\n",
    "\n",
    "\n",
    "# Subclass the HuggingFace BERTForQuestionAnswering model\n",
    "class PipelinedBertForQuestionAnswering(transformers.BertForQuestionAnswering):\n",
    "    def parallelize(self, ipu_config):\n",
    "        layer_ipu = get_layer_ipu(ipu_config[\"layers_per_ipu\"])\n",
    "        print(\"-------------------- Device Allocation --------------------\")\n",
    "        print(\"Embedding  --> IPU 0\")\n",
    "        self.bert.embeddings = poptorch.BeginBlock(\n",
    "            self.bert.embeddings, \"Embedding\", ipu_id=0\n",
    "        )\n",
    "\n",
    "        for index, layer in enumerate(self.bert.encoder.layer):\n",
    "            ipu = layer_ipu[index]\n",
    "            if index != self.config.num_hidden_layers - 1:\n",
    "                checkpoint_outputs(layer)\n",
    "            self.bert.encoder.layer[index] = poptorch.BeginBlock(\n",
    "                layer, f\"Encoder{index}\", ipu_id=ipu\n",
    "            )\n",
    "            print(f\"Encoder {index:<2} --> IPU {ipu}\")\n",
    "\n",
    "        print(f\"QA Outputs --> IPU {ipu}\")\n",
    "        self.qa_outputs = poptorch.BeginBlock(self.qa_outputs, \"QA Outputs\", ipu_id=ipu)\n",
    "        return self\n",
    "\n",
    "    # Model training loop is entirely running on IPU so we add Loss computation here\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids,\n",
    "        attention_mask,\n",
    "        token_type_ids,\n",
    "        start_positions=None,\n",
    "        end_positions=None,\n",
    "    ):\n",
    "        inputs = {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"token_type_ids\": token_type_ids,\n",
    "            \"start_positions\": start_positions,\n",
    "            \"end_positions\": end_positions,\n",
    "        }\n",
    "        output = super().forward(**inputs)\n",
    "        if self.training:\n",
    "            final_loss = poptorch.identity_loss(output.loss, reduction=\"none\")\n",
    "            return final_loss, output.start_logits, output.end_logits\n",
    "        else:\n",
    "            return output.start_logits, output.end_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc32996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT-Large configuration\n",
    "config = transformers.BertConfig(\n",
    "    hidden_size=1024,\n",
    "    intermediate_size=1024 * 4,\n",
    "    num_hidden_layers=24,\n",
    "    num_attention_heads=16,\n",
    "    hidden_dropout_prob=0.15,\n",
    "    attention_probs_dropout_prob=0.1,\n",
    "    layer_norm_eps=1e-6,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5d517b0f",
   "metadata": {},
   "source": [
    "Model is still on CPU at this point. We can use `from_pretrained` to load pretrained checkpoints from the HuggingFace Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145e3ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PipelinedBertForQuestionAnswering.from_pretrained(\n",
    "    \"Graphcore/bert-large-uncased\", config=config\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b418bd06",
   "metadata": {},
   "source": [
    "This notebook can be configured to run on 4 or 16 IPUs. On certain environments the number of available IPUs will be communicated through an environment variable. Feel free to experiment with different configurations to see how the we can produce different behaviours from different system configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9a3ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_ipus = int(os.getenv(\"NUM_AVAILABLE_IPU\", 16))\n",
    "\n",
    "assert number_of_ipus in {4, 16}, \"Only 4 or 16 IPUs are supported in this notebook\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "85307116",
   "metadata": {},
   "source": [
    "We can now set up our pipelined execution by specifying which layers to put on each IPU, and passing it to the `parallelize` method that we defined above.\n",
    "\n",
    "We also call the `.half()` method to cast all the model weights to half-precision (FP16). The `.train()` sets the PyTorch model to training mode.\n",
    "\n",
    "If you unfamiliar with training in half precision on IPU, then we have a tutorial on \"Half and Mixed Precision in PopTorch\" `../learning-PyTorch-on-IPU/mixed_precision/walkthrough.ipynb`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6df273",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipu_config = {\n",
    "    \"layers_per_ipu\": [2, 3, 3, 3, 3, 3, 3, 4]\n",
    "    if number_of_ipus == 16\n",
    "    else [3, 7, 7, 7],\n",
    "    \"recompute_checkpoint_every_layer\": True,\n",
    "}\n",
    "\n",
    "model.parallelize(ipu_config).half().train();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c97b7ceb",
   "metadata": {},
   "source": [
    "## 3. Runtime Configuration\n",
    "\n",
    "If running on an IPU-POD16, we will use a global batch size of 256 divided as such:\n",
    "- Micro batch size of 2 (the size of the batch passed to each replica)\n",
    "- Replication factor of 2 (the number of data parallel replicas, see the [Data Parallelism](#Data-Parallelism) section)\n",
    "- Gradient accumulation of 256 (the number of forward/backward passes performed per weight update)\n",
    "\n",
    "If running on an IPU-POD4, we decrease our batch size and replication factor, as we can see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d05c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "if number_of_ipus == 16:\n",
    "    micro_batch_size = 2\n",
    "    replication_factor = 2\n",
    "else:\n",
    "    micro_batch_size = 1\n",
    "    replication_factor = 1\n",
    "\n",
    "global_batch_size = 256\n",
    "gradient_accumulation = int(global_batch_size / micro_batch_size / replication_factor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "257cf362",
   "metadata": {},
   "source": [
    "`device_iterations` is the number of batches the device should run before returning to the user. Increasing `device_iterations` can be more efficient because the loop runs on the IPU directly, reducing overhead costs. Please see the [documentation](https://docs.graphcore.ai/projects/poptorch-user-guide/en/latest/batching.html#poptorch-options-deviceiterations) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3535b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_iterations = 1\n",
    "samples_per_iteration = global_batch_size * device_iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bd60ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f5263eff",
   "metadata": {},
   "source": [
    "We will now set the IPU configuration options, the default options will be configured for an IPU-POD16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f31ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_DIR = (\n",
    "    os.getenv(\"POPLAR_EXECUTABLE_CACHE_DIR\", \"/tmp/exe_cache/\") + \"/fine-tuning-bert\"\n",
    ")\n",
    "\n",
    "\n",
    "def ipu_training_options(\n",
    "    gradient_accumulation, replication_factor, device_iterations, number_of_ipus\n",
    "):\n",
    "    opts = poptorch.Options()\n",
    "    opts.randomSeed(12345)\n",
    "    opts.deviceIterations(device_iterations)\n",
    "\n",
    "    # Use Pipelined Execution\n",
    "    opts.setExecutionStrategy(\n",
    "        poptorch.PipelinedExecution(poptorch.AutoStage.AutoIncrement)\n",
    "    )\n",
    "\n",
    "    # Use Stochastic Rounding\n",
    "    opts.Precision.enableStochasticRounding(True)\n",
    "\n",
    "    # Half precision partials for matmuls and convolutions\n",
    "    opts.Precision.setPartialsType(torch.float16)\n",
    "\n",
    "    opts.replicationFactor(replication_factor)\n",
    "\n",
    "    opts.Training.gradientAccumulation(gradient_accumulation)\n",
    "\n",
    "    # Return the final result from IPU to host\n",
    "    opts.outputMode(poptorch.OutputMode.Final)\n",
    "\n",
    "    # Cache compiled executable to disk\n",
    "    opts.enableExecutableCaching(CACHE_DIR)\n",
    "\n",
    "    # Setting system specific options\n",
    "\n",
    "    # On-chip Replicated Tensor Sharding of Optimizer State\n",
    "    opts.TensorLocations.setOptimizerLocation(\n",
    "        poptorch.TensorLocationSettings()\n",
    "        # Optimizer state lives on IPU if running on a POD16\n",
    "        .useOnChipStorage(number_of_ipus == 16)\n",
    "        # Optimizer state sharded between replicas with zero-redundancy\n",
    "        .useReplicatedTensorSharding(True)\n",
    "    )\n",
    "\n",
    "    # Available Transient Memory For matmuls and convolutions operations dependent on system type\n",
    "    if number_of_ipus == 16:\n",
    "        amps = [0.08, 0.28, 0.32, 0.32, 0.36, 0.38, 0.4, 0.1]\n",
    "    else:\n",
    "        amps = [0.15, 0.18, 0.2, 0.25]\n",
    "\n",
    "    opts.setAvailableMemoryProportion({f\"IPU{i}\": mp for i, mp in enumerate(amps)})\n",
    "\n",
    "    ## Advanced performance options ##\n",
    "\n",
    "    # Only stream needed tensors back to host\n",
    "    opts._Popart.set(\"disableGradAccumulationTensorStreams\", True)\n",
    "\n",
    "    # Copy inputs and outputs as they are needed\n",
    "    opts._Popart.set(\n",
    "        \"subgraphCopyingStrategy\", int(popart.SubgraphCopyingStrategy.JustInTime)\n",
    "    )\n",
    "\n",
    "    # Parallelize optimizer step update\n",
    "    opts._Popart.set(\n",
    "        \"accumulateOuterFragmentSettings.schedule\",\n",
    "        int(popart.AccumulateOuterFragmentSchedule.OverlapMemoryOptimized),\n",
    "    )\n",
    "    opts._Popart.set(\"accumulateOuterFragmentSettings.excludedVirtualGraphs\", [\"0\"])\n",
    "\n",
    "    # Limit number of sub-graphs that are outlined (to preserve memory)\n",
    "    opts._Popart.set(\"outlineThreshold\", 10.0)\n",
    "\n",
    "    # Only attach to IPUs after compilation has completed.\n",
    "    opts.connectionType(poptorch.ConnectionType.OnDemand)\n",
    "    return opts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1325da",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_opts = ipu_training_options(\n",
    "    gradient_accumulation, replication_factor, device_iterations, number_of_ipus\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "19638809",
   "metadata": {},
   "source": [
    "## 4. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cf239b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from squad_preprocessing import PadCollate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67e431c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 384"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d4aa9ad1",
   "metadata": {},
   "source": [
    "We use the `poptorch.Dataloader` which is a drop-in replacement of `torch.Dataloader`, extending it with IPU specific options and asynchronous execution.\n",
    "\n",
    "Another small difference is the use of the `PadCollate` function. If `drop_last` option is false in a `DataLoader`, you may end up with a *remainder mini-batch* at the end of the epoch that is smaller than the other mini-batches. \n",
    "\n",
    "For a compiled device like IPU that would require a recompilation of the execution graph for that one mini-batch. In order to not lose any training examples, we can pad the remainder mini-batch with zeros and set the target values to a special value which will set the loss to 0 in those cases so they don't affect training. This way we have consistent mini-batch sizes and we can train on all the data.\n",
    "\n",
    "More information can be found in the [`poptorch.DataLoader` documentation.](https://docs.graphcore.ai/projects/poptorch-user-guide/en/latest/batching.html#efficient-data-batching) and also in our tutorial on efficient data loading - `../learning-PyTorch-on-IPU/efficient_data_loading/walkthrough.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b1ed83",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = poptorch.DataLoader(\n",
    "    train_opts,\n",
    "    train_dataset,\n",
    "    batch_size=micro_batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    "    collate_fn=PadCollate(\n",
    "        samples_per_iteration,\n",
    "        {\n",
    "            \"input_ids\": 0,\n",
    "            \"attention_mask\": 0,\n",
    "            \"token_type_ids\": 0,\n",
    "            \"start_positions\": sequence_length,\n",
    "            \"end_positions\": sequence_length,\n",
    "        },\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fe654bbd",
   "metadata": {},
   "source": [
    "We will use poptorch's version of the `AdamW` optimizer, its interface is the same as `AdamW` from `torch.optim`, but with some extra options.\n",
    "\n",
    "Please see the [`poptorch.optim` documentation](https://docs.graphcore.ai/projects/poptorch-user-guide/en/latest/overview.html#optimizers) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658086c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(model):\n",
    "    # Do not apply weight_decay for one-dimensional parameters\n",
    "    regularized_params = []\n",
    "    non_regularized_params = []\n",
    "    for param in model.parameters():\n",
    "        if param.requires_grad:\n",
    "            if len(param.shape) == 1:\n",
    "                non_regularized_params.append(param)\n",
    "            else:\n",
    "                regularized_params.append(param)\n",
    "\n",
    "    params = [\n",
    "        {\"params\": regularized_params, \"weight_decay\": 0.01},\n",
    "        {\"params\": non_regularized_params, \"weight_decay\": 0},\n",
    "    ]\n",
    "    optimizer = poptorch.optim.AdamW(\n",
    "        params,\n",
    "        lr=1e-4,\n",
    "        weight_decay=0,\n",
    "        eps=1e-6,\n",
    "        bias_correction=True,\n",
    "        loss_scaling=64,\n",
    "        first_order_momentum_accum_type=torch.float16,\n",
    "        accum_type=torch.float16,\n",
    "    )\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da77c23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = get_optimizer(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "475f67b7",
   "metadata": {},
   "source": [
    "Now we create the function that will train our model on the IPU, then run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6af0158",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(model, opts, optimizer, train_dl, num_epochs):\n",
    "    num_steps = num_epochs * len(train_dl)\n",
    "    lr_scheduler = transformers.get_scheduler(\n",
    "        \"cosine\", optimizer, 0.1 * num_steps, num_steps\n",
    "    )\n",
    "\n",
    "    # Wrap the pytorch model with poptorch.trainingModel\n",
    "    training_model = poptorch.trainingModel(model, train_opts, optimizer)\n",
    "\n",
    "    # Compile model or load from executable cache\n",
    "    batch = next(iter(train_dl))\n",
    "    outputs = training_model.compile(\n",
    "        batch[\"input_ids\"],\n",
    "        batch[\"attention_mask\"],\n",
    "        batch[\"token_type_ids\"],\n",
    "        batch[\"start_positions\"],\n",
    "        batch[\"end_positions\"],\n",
    "    )\n",
    "    # Training Loop\n",
    "    for epoch in trange(num_epochs, desc=\"Epochs\"):\n",
    "        train_iter = tqdm(train_dl)\n",
    "        for step, batch in enumerate(train_iter):\n",
    "            start_step = time.perf_counter()\n",
    "\n",
    "            # This completes a forward+backward+weight update step\n",
    "            outputs = training_model(\n",
    "                batch[\"input_ids\"],\n",
    "                batch[\"attention_mask\"],\n",
    "                batch[\"token_type_ids\"],\n",
    "                batch[\"start_positions\"],\n",
    "                batch[\"end_positions\"],\n",
    "            )\n",
    "\n",
    "            # Update the LR and update the poptorch optimizer\n",
    "            lr_scheduler.step()\n",
    "            training_model.setOptimizer(optimizer)\n",
    "            step_length = time.perf_counter() - start_step\n",
    "            step_throughput = samples_per_iteration / step_length\n",
    "            loss = outputs[0].mean().item()\n",
    "            train_iter.set_description(\n",
    "                f\"Epoch: {epoch} - \"\n",
    "                f\"Step: {step} - \"\n",
    "                f\"Loss: {loss:3.3f} - \"\n",
    "                f\"Throughput: {step_throughput:3.3f} seq/s\"\n",
    "            )\n",
    "\n",
    "    # Detach the model from the device once training is over so the device is free to be reused for validation\n",
    "    training_model.detachFromDevice()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57650618",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer(model, train_opts, optimizer, train_dl, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58477388",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d0d6765f",
   "metadata": {},
   "source": [
    "After training, we save the model weights to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4c968e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_length = len(ipu_config[\"layers_per_ipu\"])\n",
    "checkpoint_name = f\"checkpoints/squad_large_{replication_factor}x{pipeline_length}\"\n",
    "model.save_pretrained(checkpoint_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d7b22b6e",
   "metadata": {},
   "source": [
    "## 5. Validation\n",
    "\n",
    "We will now take the model we just trained on the training data and run validation on the SQuAD validation dataset. The model will run on a 2-IPU pipeline that we will replicate 4 times if running on an IPU-POD16, or 2 times for an IPU-POD4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae59fbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "from squad_preprocessing import postprocess_qa_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a491a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "micro_batch_size = 4\n",
    "replication_factor = 4 if number_of_ipus == 16 else 2\n",
    "global_batch_size = micro_batch_size * replication_factor\n",
    "device_iterations = 2\n",
    "samples_per_iteration = global_batch_size * device_iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1680e797",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ipu_validation_options(replication_factor, device_iterations):\n",
    "    opts = poptorch.Options()\n",
    "    opts.randomSeed(42)\n",
    "    opts.deviceIterations(device_iterations)\n",
    "\n",
    "    opts.setExecutionStrategy(\n",
    "        poptorch.PipelinedExecution(poptorch.AutoStage.AutoIncrement)\n",
    "    )\n",
    "\n",
    "    # Stochastic rounding not needed for validation\n",
    "    opts.Precision.enableStochasticRounding(False)\n",
    "\n",
    "    # Half precision partials for matmuls and convolutions\n",
    "    opts.Precision.setPartialsType(torch.float16)\n",
    "\n",
    "    opts.replicationFactor(replication_factor)\n",
    "\n",
    "    # No gradient accumulation for Inference\n",
    "    opts.Training.gradientAccumulation(1)\n",
    "\n",
    "    # Return all results from IPU\n",
    "    opts.outputMode(poptorch.OutputMode.All)\n",
    "\n",
    "    # Cache compiled executable to disk\n",
    "    opts.enableExecutableCaching(CACHE_DIR)\n",
    "\n",
    "    return opts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b2737f",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_opts = ipu_validation_options(replication_factor, device_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ee0a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipu_config = {\"layers_per_ipu\": [11, 13], \"recompute_checkpoint_every_layer\": False}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "af9dbe31",
   "metadata": {},
   "source": [
    "Let's load the model weights we previously trained from disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f2c580",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PipelinedBertForQuestionAnswering.from_pretrained(checkpoint_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c5c05c87",
   "metadata": {},
   "source": [
    "We cast the model weights to half precision (FP16) and set the model to evaluation mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2becb145",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.parallelize(ipu_config).half().eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55e3159",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dl = poptorch.DataLoader(\n",
    "    val_opts,\n",
    "    validation_features.remove_columns([\"example_id\", \"offset_mapping\"]),\n",
    "    batch_size=micro_batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    collate_fn=PadCollate(\n",
    "        samples_per_iteration,\n",
    "        {\"input_ids\": 0, \"attention_mask\": 0, \"token_type_ids\": 0},\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6e638b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validator(model, opts, val_dl):\n",
    "    # Wrap the pytorch model with poptorch.inferenceModel for inference\n",
    "    inference_model = poptorch.inferenceModel(model, opts)\n",
    "\n",
    "    raw_predictions = [[], []]\n",
    "    val_iter = tqdm(val_dl, desc=\"Validation\")\n",
    "    for step, batch in enumerate(val_iter):\n",
    "        start_step = time.perf_counter()\n",
    "        outputs = inference_model(\n",
    "            batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"token_type_ids\"]\n",
    "        )\n",
    "        step_length = time.perf_counter() - start_step\n",
    "        step_throughput = samples_per_iteration / step_length\n",
    "        raw_predictions[0].append(outputs[0])\n",
    "        raw_predictions[1].append(outputs[1])\n",
    "        val_iter.set_description(\n",
    "            f\"Step: {step} - throughput: {step_throughput:3.3f} samples/s\"\n",
    "        )\n",
    "    inference_model.detachFromDevice()\n",
    "\n",
    "    raw_predictions[0] = torch.vstack(raw_predictions[0]).float().numpy()\n",
    "    raw_predictions[1] = torch.vstack(raw_predictions[1]).float().numpy()\n",
    "    return raw_predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "41499e1e",
   "metadata": {},
   "source": [
    "We loop over all the validation data examples and get the `raw_predictions` for the start and end positions of where the answer to the question lies in the text passage for each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd938777",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_predictions = validator(model, val_opts, val_dl)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1ff9a2ca",
   "metadata": {},
   "source": [
    "We now post-processed the raw predictions to the question answering task to get the best prediction that's valid for each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db39ed84",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predictions = postprocess_qa_predictions(\n",
    "    datasets[\"validation\"], validation_features, raw_predictions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e325c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric(\"squad\")\n",
    "formatted_predictions = [\n",
    "    {\"id\": k, \"prediction_text\": v} for k, v in final_predictions.items()\n",
    "]\n",
    "references = [\n",
    "    {\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in datasets[\"validation\"]\n",
    "]\n",
    "metrics = metric.compute(predictions=formatted_predictions, references=references)\n",
    "print(metrics)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2743c783",
   "metadata": {},
   "source": [
    "We obtained here a good validation score for SQuADv1.\n",
    "\n",
    "| BERT-Large                             | Exact Match | F1 Score |\n",
    "|----------------------------------------|:-----------:|:--------:|\n",
    "| Reference (Devling et al. 2018)        | 84.1        | 90.9     |\n",
    "| IPU-POD16 with IPU pre-trained weights | 84.7        | 91.1     |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "be811951",
   "metadata": {},
   "source": [
    "## 6. Inference\n",
    "\n",
    "We can now use our fine-tuned model to answer questions. Let's start by defining a task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe37992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define task\n",
    "question = (\n",
    "    \"What speed-up can one expect from using sequence packing for training BERT on IPU?\"\n",
    ")\n",
    "answer_text = (\n",
    "    \"We find that at sequence length 512 padding tokens represent in excess of 50% of the Wikipedia\"\n",
    "    \"dataset used for pretraining BERT (Bidirectional Encoder Representations from Transformers).\"\n",
    "    \"Therefore by removing all padding we achieve a 2x speed-up in terms of sequences/sec.\"\n",
    "    \"To exploit this characteristic of the dataset,\"\n",
    "    \"we develop and contrast two deterministic packing algorithms.\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aada5a30",
   "metadata": {},
   "source": [
    "Let's get the model inputs ready and create our model. We'll import the weights from the pre-trained, fine-tuned BERT model from the previous sections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f610ea8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the tokenizer to the input text, treating them as a text-pair.\n",
    "input_encoding = tokenizer.encode_plus((question, answer_text))\n",
    "\n",
    "# Extract inputs, add batch dimension\n",
    "input_tensor = torch.tensor(input_encoding[\"input_ids\"]).unsqueeze(0)\n",
    "attention_tensor = torch.tensor(input_encoding[\"attention_mask\"]).unsqueeze(0)\n",
    "token_types = torch.tensor(input_encoding[\"token_type_ids\"]).unsqueeze(0)\n",
    "\n",
    "# Get model and load the fine-tuned weights\n",
    "model = transformers.BertForQuestionAnswering.from_pretrained(checkpoint_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c13649ee",
   "metadata": {},
   "source": [
    "Optionally, instead of using the fine-tuned weights we saved in the previous section, you can download fine-tuned weights from the [Graphcore organisation on the HuggingFace Model Hub](https://huggingface.co/Graphcore). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b79c815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = transformers.BertForQuestionAnswering.from_pretrained(\"Graphcore/bert-large-uncased-squad11\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f3ba2316",
   "metadata": {},
   "source": [
    "We can now solve the task and print the answer to the question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92cd64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve task\n",
    "outputs = model(input_tensor, attention_tensor, token_types)\n",
    "\n",
    "# Extract answer\n",
    "answer_start, answer_stop = outputs.start_logits.argmax(), outputs.end_logits.argmax()\n",
    "answer_ids = input_tensor.squeeze()[answer_start : answer_stop + 1]\n",
    "answer_tokens = tokenizer.convert_ids_to_tokens(answer_ids, skip_special_tokens=True)\n",
    "answer = tokenizer.convert_tokens_to_string(answer_tokens)\n",
    "\n",
    "# Print results\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "49a24459",
   "metadata": {},
   "source": [
    "**That's it for today folks. We learned how to fine-tune a BERT-Large model on the SQuADv1.1 dataset to get SOTA performance. Our model can now answer questions.**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
